#!/usr/bin/env python3
# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.
#
import os
import sys
import shutil
import pprint
import logging
import argparse
import platform
import subprocess
import xml.etree.ElementTree as ET

from urllib.request import urlopen
from pathlib import Path
from enum import Enum


__version__='0.1.0'


###############################################################

def cli_arguments():
    """ 
    Return a command-line parser with the following arguments. 
    """
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('command', type=str, choices=['download', 'serve'], nargs='*', default='serve')
    parser.add_argument('params', type=str, nargs='*')

    parser.add_argument('--model', type=str, default=None, help="URL or local path to the model")
    parser.add_argument('--dataset', type=str, default=None, help="URL or path to datasets for when they are used")
    parser.add_argument('--api', type=str, default='mlc', choices=['mlc', 'trt_llm', 'vllm', 'hf', 'llama.cpp'])
    parser.add_argument('--api-key', type=str, default=Env.HF_TOKEN, help="personal access token for gated/private models (either HuggingFace Hub, NVIDIA NGC)")
    parser.add_argument('--quantization', type=str, default='q4f16_ft', help="the type of quantization to apply to the model (this is often API specific)") #, choices=['q4f16_ft', 'q4f16_1'])
    parser.add_argument('--chat-template', type=str, default=None)

    parser.add_argument('--cache', type=str, default=Env.CACHE)
    parser.add_argument('--cache-hf', type=str, default=None)
    parser.add_argument('--cache-mlc', type=str, default=None)
    parser.add_argument('--cache-trt', type=str, default=None)

    parser.add_argument('--max-context-len', type=int, default=None)
    parser.add_argument('--max-batch-size', type=int, default=None)
    parser.add_argument('--prefill-chunk', type=int, default=None)

    parser.add_argument('--host', type=str, default='0.0.0.0')
    parser.add_argument('--port', type=int, default=9000)
    
    parser.add_argument('--version', action='store_true', help='print system/environment info')
    parser.add_argument('--verbose', action='store_true')
    parser.add_argument('--log-level', default=None, type=str, choices=['debug', 'info', 'warning', 'error', 'critical'])

    return parser

###############################################################

def setup_environment():
    """ 
    Configure environment variables and device enumeration.
    """
    Env = Environment()
    gpu = gpu_device_query()

    Env.CPU_ARCH = platform.machine()
    Env.NUM_CPU = os.cpu_count()
    Env.NUM_GPU = gpu.get('attached_gpus', 0)
    Env.CUDA_VERSION = gpu.get('cuda_version')
    Env.NVIDIA_DRIVER = gpu.get('driver_version')
    Env.GPU = gpu.get('gpu')

    '''
    Env.CONTAINERS = {
        'jp61': {
            'hf': "dustynv/llama-factory:r36.4.0",
            'mlc': "dustynv/mlc:0.19.0-r36.4.0",
            'vllm': "dustynv/vllm:0.6.6.post1-r36.4.0",
            'trt_llm': "dustynv/tensorrt_llm:0.12.0-r36.4.0",
        }
    }
    '''
    
    Env.CACHE = str(Path(
        getenv(
            ['HF_HUB_CACHE', 'TRANSFORMERS_CACHE', 'HF_HOME'], 
            '~/.cache/huggingface'
        )
    ).expanduser().parent)

    Env.HF_TOKEN = getenv(['HF_TOKEN', 'HUGGINGFACE_TOKEN'])
    Env.HAS_MLC = has_command('mlc_llm')
    Env.HAS_HF_HUB = has_import('huggingface_hub')
    Env.HAS_NVIDIA_SMI = has_command('nvidia-smi')

    Env.HAS_DOCKER_API=has_import('docker')
    Env.HAS_DOCKER_CLI=has_command('docker')

    #HAS_TRT=has_import('tensorrt')
    #HAS_TRT_LLM=has_import('tensorrt_llm')
    #HAS_TRANSFORMERS=has_import('transformers')

    return Env

###############################################################

class Environment(dict):
    """ Environment variables """
    def __init__(self, *args, **kwargs):
        dict.__init__(self, *args, **kwargs)

    def __getattr__(self, key):
        return self[key]

    def __setattr__(self, key, value):
        self[key] = value

    def __getstate__(self):
        return self.__dict__

    def __setstate__(self, value):
        self.__dict__ = value

    def format(self, obj=None, key=None, sep='#', column=22, width=60, margin=2, indent=2, level=0):
        """ Print a nested tree of objects with aligned columns """
        txt = ''
        mse = ' ' * margin + sep
        pad = mse + ' ' * (indent * level)

        def pad_row(text):
            return text.ljust(width+1) + sep + '\n'

        if level <= 1:    sym = ''
        elif level == 2:  sym = '* '
        else:             sym = '- '

        if not obj:
            if level > 0:
                return txt
            obj = self

        if not key:
            for key in obj:
                txt += self.format(obj=obj[key], key=key, sep=sep, column=column, 
                        width=width, margin=margin, indent=indent, level=level+1)
            div = mse + sep * (width-1) + '\n'
            blank = mse.ljust(width+1) + '#\n'
            return ''.join([div,
                pad_row(f"{mse} GPU Device Configuration "),
                div, blank, txt, blank, div,
            ])

        if isinstance(obj, dict):
            if key:
                txt += pad_row(f'{pad}{sym}{key}')
            for x in obj:
                txt += self.format(obj=obj[x], key=x, sep=sep, column=column, 
                     width=width, margin=margin, indent=indent, level=level+1)
        elif isinstance(obj, list) and isinstance(obj[0], (dict, list)):
            if len(obj) > 1:
                if key:
                    txt += pad_row(f'{pad}{sym}{key}'.ljust(column))
                for x in obj:
                    txt += self.format(obj=obj[x], key=x, sep=sep, column=column, 
                             width=width, margin=margin, indent=indent, level=level+1)
            else:
                txt += pad_row(f'{pad+sym+key}'.ljust(column) + str(obj[0]))  
        else:
            txt += pad_row(f"{pad+sym+key}".ljust(column) + str(obj))

        return txt

###############################################################

class ColorLog(logging.Formatter):

    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

    grey = "\x1b[38;20m"
    yellow = "\x1b[33;20m"
    red = "\x1b[31;20m"
    bold_red = "\x1b[31;1m"
    reset = "\x1b[0m"

    format = "%(asctime)s - %(message)s"
    datefmt = "%Y-%m-%d %H:%M:%S"

    FORMATS = {
        logging.DEBUG: grey + format + reset,
        logging.INFO: grey + format + reset,
        logging.WARNING: yellow + format + reset,
        logging.ERROR: red + format + reset,
        logging.CRITICAL: bold_red + format + reset
    }

    @staticmethod
    def setup(name=__name__):
        level = getattr(logging, getenv('LOG_LEVEL', 'info').upper(), logging.INFO)
        
        logging.basicConfig(
            stream=sys.stderr, 
            level=level,
            format='%(asctime)s - %(message)s', datefmt="%Y-%m-%d %H:%M:%S"
        )
        logger = logging.getLogger(name)
        logging.getLogger().removeHandler(logging.getLogger().handlers[0])
        #logger.handlers.clear()
        ch = logging.StreamHandler()
        ch.setLevel(level)
        ch.setFormatter(ColorLog())
        logger.addHandler(ch)
        return logger
    
    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)
         
###############################################################

def shell(cmd, echo=True, capture_output=False, **kwargs):
    """ 
    Run shell command and return the result 
    """
    if not isinstance(cmd, list):
        cmd = [cmd]
        
    if echo:
        endline = f' \\\n    {ColorLog.OKGREEN}'
        echo = echo if isinstance(echo, str) else 'Running shell command'
        logger.info(f"{echo}:\n\n  {ColorLog.OKGREEN}{endline.join(cmd)}{ColorLog.reset}\n")

    kwargs.setdefault('executable', '/bin/bash')
    kwargs.setdefault('shell', True)
    kwargs.setdefault('check', True)
    kwargs.setdefault('capture_output', capture_output)
    kwargs.setdefault('text', capture_output)

    return subprocess.run(' '.join(cmd), **kwargs)

def subshell(cmd, capture_output=True, **kwargs):
    """ 
    Run a shell and capture the output by default
    """
    return shell(cmd, capture_output=capture_output, **kwargs).stdout

def has_command(exe):
    """
    Return true if there's an executable found in the PATH by this name. 
    """
    return shutil.which(exe) is not None

def has_import(module):
    """ 
    Return true if import succeeds, false otherwise 
    """
    try:
        __import__(module)
        return True
    except ImportError as error:
        logger.debug(f"{module} not found ({error})")
        return False

def gpu_device_query():
    """ 
    Get GPU device info from nvidia-smi 
    """
    try:
        return xmlToJson(subshell('/usr/sbin/nvidia-smi -q -x', echo='Running nvidia-smi query'))
    except Exception as error:
        logger.warning(f'Failed to query GPU devices from nvidia-smi ({error})')
    
def getenv(keys, default=None):
    """ 
    Get environment variable from the OS with fallback options.
    """
    if not isinstance(keys, list):
        keys = [keys]

    while len(keys) > 0:
        env = keys.pop(0)
        if env in os.environ:
            return os.environ[env] 
        
    return default
     
###############################################################

class Docker:
    """
    Some basic utilities for starting/stopping containers
    This requires the docker socket to be mounted:
      /var/run/docker.sock:/var/run/docker.sock
    """
    Client = None

    @staticmethod
    def client():
        if not Docker.Client:
            Docker.Client = docker.from_env()
        return Docker.Client
    
    @staticmethod
    def stop(name):
        try:
            Docker.client().containers.get(name).stop()
        except Exception as error:
            logging.error(f"Failed to stop container '{name}' ({error})")
            Docker.kill(name)
        
    @staticmethod
    def kill(name):
        Docker.client().containers.get(name).kill()

###############################################################

def xmlToJson(tree, nan=[], blacklist=[], rename={}):
    """ 
    Convert XML to JSON and filter the keys.
    """
    response = {}

    if not nan:
        nan = ['N/A', 'Unknown Error', 'None', None]

    if not blacklist:
        blacklist = [
            'gpu_reset_status', 'ibmnpu', 'temperature',
            'gpu_power_readings', 'module_power_readings'
        ]

    if not rename:
        rename = {
            'product_name': 'name',
            'product_architecture': 'arch',
        }

    def is_nan(text):
        text = text.lower()
        for n in nan:
            if n:
                if n.lower() in text:
                    return True
            else:
                if not text:
                    return True
        return False
    
    if isinstance(tree, str):
        tree = ET.fromstring(tree)

    for child in tree:
        if child.tag in blacklist:
            continue
        if child.tag in rename:
            child.tag = rename[child.tag]

        if len(list(child)) > 0:
            children = xmlToJson(child)
            if children:
                if child.tag in response:
                    if isinstance(response[child.tag], list):
                        response[child.tag].append(children)
                    else:
                        response[child.tag] = [response[child.tag], children]
                else:      
                    response[child.tag] = children
        else:
            text = child.text.strip()
            if not is_nan(text):
                response[child.tag] = text

    return response

###############################################################

def download_model(model: str, cache: str=None, api_key: str=None, flatten=False, **kwargs):
    """
    Download a model repo or file from HuggingFace Hub

    For now we are assuming HF API is available,
    but this should move to launching downloader in docker.
    """
    if not Env.HAS_HF_HUB:
        raise ImportError(f"Attempted to use huggingface_hub without it being installed first")
    
    from huggingface_hub import hf_hub_download, snapshot_download

    cache = os.path.join(
        cache, model.replace('/', '--') 
        if flatten else model
    )

    os.makedirs(cache, mode=0o755, exist_ok=True)

    kwargs.setdefault('resume_download', True)
    kwargs.setdefault('repo_type', 'model')

    # Handle either "org/repo" or individual "org/repo/file"
    # the former has 0-1 slashes, while the later has 2.
    num_slashes = 0
    
    for c in model:
        if c == '/':
            num_slashes += 1
            
    if num_slashes >= 2:  
        slash_count = 0
        
        for idx, i in enumerate(model):
            if i == '/':
                slash_count += 1
                if slash_count == 2:
                    break
                    
        repo_id = model[:idx]
        filename = model[idx+1:]

        logger.info(f"Downloading file {filename} from HF Hub - {model}")
        repo_path = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=cache, token=api_key, **kwargs)
    else:
        logger.info(f"Downloading model from HF Hub - {model}")
        repo_path = snapshot_download(repo_id=model, local_dir=cache, token=api_key, **kwargs)

    logger.info(f"Downloaded {model} to:\n    {repo_path}\n")
    return repo_path

def hf_hub_exists(model: str, api_key: str=None, **kwargs):
    """
    Check if a model repo exists / is accessible on HF Hub or not.
    """
    if not Env.HAS_HF_HUB:
        raise ImportError(f"Attempted to use huggingface_hub without it being installed first")
    
    from huggingface_hub import model_info

    try:
        info = model_info(model, token=api_key)
        logger.debug(f"{model} | downloaded model info:\n{pprint.pformat(info, indent=2)}")
    except Exception as error:
        logger.error(f"{model} | could not find or access this model on HF Hub ({error})")
        return False
    
    return True

def get_chat_template(model):
    """
    Fallback to best estimate the model's conversation or tokenization template.
    """
    name = model.lower()
            
    if 'llama' in name:   
        if '-2-' in name:
            return "llama-2"
        return "llama-3_1"
    elif 'qwen' in name:
        return 'qwen2'
    elif 'phi-3' in name:
        return 'phi-3'
    elif 'smol' in name:
        return 'chatml'

    logger.warning(f"{model} | a default chat template wasn't found, please set it with --chat-template")    
    return None

###############################################################

class MLC:
    """
    MLC/TVM deployment - download, quantize, generate config, compile, serve
    """
    @staticmethod
    def deploy(model: str=None, quantization: str=None, **kwargs):
        model_path = Path(model)

        if model_path.is_dir():
            model_lib = MLC.find_model_lib(model_path)

            if model_lib:
                return MLC.serve(model_lib, quantization=quantization, **kwargs)

            quant_path = [x for x in Path(model_path).glob('**/params_*.bin')]
            quant_path = os.path.dirname(quant_path[0]) if quant_path else None
            quantized = bool(quant_path)

            if not quantized:
                quant_path = MLC.quantize(path, quantization=quantization, **kwargs)
            
            has_mlc_config = model_path.joinpath('mlc-chat-config.json').exists()

            if not quantized or not has_mlc_config:
                MLC.config(model_path, quant_path, quantization=quantization, **kwargs)

            model_lib = MLC.compile(quant_path, quantization=quantization, **kwargs)        
        else:
            if len(model_path.parts) != 2:
                raise ValueError(f"Invalid local path or remote URL, or resource not found ({model_path})")
            
            model_org, model_name = Path(model).parts
            is_quant = quantization in model_name and '-mlc' in model_name.lower()

            if not is_quant:
                quant_model = f'{model_name}-{quantization}-MLC'
                quant_hosts = ['dusty-nv', 'mlc-ai']
                for quant_host in quant_hosts:
                    quant_repo = os.path.join(quant_host, quant_model)
                    if hf_hub_exists(quant_repo, **kwargs):
                        return MLC.deploy(quant_repo, quantization=quantization, **kwargs)
                    
            if not hf_hub_exists(model, **kwargs):
                raise IOError(f"could not locate or access model {model}")
            
            cache_key = 'cache_mlc' if is_quant else 'cache_hf'
            model_path = download_model(model, cache=kwargs.get(cache_key), api_key=kwargs.get('api_key'))
            return MLC.deploy(model=model_path, quantization=quantization, **kwargs)

        return MLC.serve(model_lib, quantization=quantization, **kwargs)

    @staticmethod
    def quantize(model_path : str, quantization: str=None, cache_mlc: str=None, **kwargs):
        quant_path = os.path.join(cache_mlc, os.path.basename(model_path)) + f"-{quantization}-MLC"
        cmd = [
            f'mlc_llm convert_weight --quantization {quantization}',
            f"{model_path}",
            f"--output {quant_path}"
        ]
        shell(cmd, echo='Running MLC quantization')
        return quant_path
    
    @staticmethod
    def config(model_path : str, quant_path : str, quantization: str=None, **kwargs):
        kwargs.setdefault('chat_template', get_chat_template(model_path))
        cmd = [f'mlc_llm gen_config --quantization {quantization}']
        cmd += MLC.overrides(packed=False, **kwargs)
        cmd += [f'--output {quant_path}', f'{model_path}']
        shell(cmd, echo='Generating MLC configuration')
        return quant_path

    @staticmethod
    def compile(quant_path : str, **kwargs):
        model_lib = os.path.join(quant_path, f"model.so")
        cmd = [f"mlc_llm compile --device cuda --opt O3"]
        cmd += MLC.overrides(**kwargs)
        cmd += [f"{quant_path}", f"--output {model_lib}"]
        shell(cmd, echo='Compiling MLC model')
        return model_lib

    @staticmethod
    def serve(model_lib : str, quantization: str=None, host: str='0.0.0.0', port: int=9000, max_batch_size: int=1, cache_mlc: str=None, **kwargs):
        model_lib = Path(model_lib)
        model_lib = model_lib.relative_to(cache_mlc)
        mode = 'local' if max_batch_size > 1 else 'interactive'
        cmd = [f"mlc_llm serve --mode {mode} --device cuda",
               f"--host {host} --port {port}"]
        cmd += MLC.overrides(exclude=['max_batch_size'], **kwargs)
        cmd += [f"--model-lib {model_lib}", f"{model_lib.parent}"]
        return shell(cmd, cwd=cache_mlc, echo='Loading model')

    @staticmethod
    def overrides(packed=True, exclude=[], **kwargs):
        overrides = {'tensor_parallel_shards': Env.NUM_GPU}

        for k,v in kwargs.items():
            if v and k in MLC.CONFIG_MAP and k not in exclude:
                overrides[MLC.CONFIG_MAP[k]] = v

        if not overrides:
            return []
               
        if packed:
            overrides = ';'.join([f'{k}={v}' for k,v in overrides.items()])
            return [f"--overrides='{overrides}'"]
        else:
            return [f"--{k.replace('_', '-')} {v}" for k,v in overrides.items()]

    @staticmethod
    def find_model_lib(model):
        path = Path(model)
        if path.is_dir():
            so = [x for x in path.glob('**/*.so')]
            return so[0] if so else None
         
    # argument name mapping
    CONFIG_MAP = {
        'chat_template': 'conv_template',
        'max_batch_size': 'max_batch_size',
        'max_context_len': 'context_window_size',
        'prefill_chunk': 'prefill_chunk_size'
    }

###############################################################

if __name__ == "__main__":

    logger = ColorLog.setup()
    Env = setup_environment()
    args = cli_arguments().parse_args()

    if args.verbose:
        args.log_level = 'debug'

    if args.log_level:
        logger.getLogger(__name__).setLevel(
            getattr(logging, args.log_level.upper(), logging.INFO)
        )

    if not args.cache_hf:
        args.cache_hf = os.path.join(args.cache, 'huggingface')
    if not args.cache_mlc:
        args.cache_mlc = os.path.join(args.cache, 'mlc_llm')
    if not args.cache_trt:
        args.cache_trt = os.path.join(args.cache, 'trt_llm')

    if not args.model and not args.dataset and not args.version:
        raise ValueError(f"Missing required argument:  --model or --dataset")
    
    if len(args.command) == 1:
        args.command = args.command[0]

    if args.command == 'download':
        asset, repo_type = (args.dataset, 'dataset') if args.dataset else (args.model, 'model')
        location = download_model(
            asset, cache=args.cache_hf, 
            api_key=args.api_key, repo_type=repo_type
        )
        logger.info(f"Downloaded {repo_type} {asset} to {location}")
        sys.exit()

    logger.info(args)
    logger.info(f'sudonim version {__version__}\n\n{ColorLog.OKGREEN + Env.format() + ColorLog.reset}\n')

    if args.version:
        sys.exit()

    if Env.HAS_MLC and args.api == 'mlc':
        MLC.deploy(**vars(args))
    else:
        raise RuntimeError(f"Unsupported configuration or model selected")
    
    '''
    elif HAS_TRT_LLM and args.api == 'trt_llm':
        TRT_LLM(args)
    elif HAS_TRANSFORMERS and args.api == 'hf':
        HF(args)
    else:
        run_container(args)
    '''
    